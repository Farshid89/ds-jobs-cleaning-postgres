# ğŸ§¹ Data Science Jobs â€” Cleaning Pipeline (PostgreSQL)

![PostgreSQL](https://img.shields.io/badge/PostgreSQL-12%2B-blue)
![ETL](https://img.shields.io/badge/Process-ETL%20Cleaning-success)
![Regex](https://img.shields.io/badge/Regex-Salary%20Parsing-orange)
![Portfolio](https://img.shields.io/badge/Project-Portfolio-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

This project demonstrates a **full SQL cleaning pipeline** applied to a messy Glassdoor-style Data Science jobs dataset.  
It covers staging, deduplication, regex salary extraction, text normalization, splitting location/HQ fields, placeholder handling, revenue bucketing, and producing a slim typed table for analysis.

---

## âœ¨ Highlights
- **Staging workflow** â†’ raw data stays untouched  
- **Duplicate removal** using `ROW_NUMBER()`  
- **Regex salary parsing** â†’ `min_salary`, `max_salary`, `avg_salary`  
- **Feature engineering**:
  - Job family classification (`simple_job`)
  - Location split: `location_city`, `location_state`
  - HQ split: `headquarter_city`, `headquarter_state`, `headquarter_country`
  - Revenue categorized into **Aâ€“L** (Z = Unknown)
- **Data quality fixes**:
  - Convert `-1` and `Unknown` placeholders to `NULL`
  - Remove embedded newlines in company names
  - Normalize size field

---
## ğŸ“‚ Repository Structure
ds-jobs-cleaning-postgres/
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ sql/
â”‚ â”œâ”€â”€ cleaning.sql # main cleaning script
â”‚ â””â”€â”€ quality_checks.sql # validation queries
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/Uncleaned_DS_jobs.csv
â”‚ â””â”€â”€ clean/cleaned_ds_jobs_me.csv
â””â”€â”€ images/
â”œâ”€â”€ before.png
â””â”€â”€ after.png


---

## ğŸ“Š Before vs After

**Raw data (before cleaning):**

![Before cleaning](images/before.png)

**Cleaned data (after cleaning):**

![After cleaning](images/after.png)

---

## â–¶ï¸ How to Run (PostgreSQL 12+)

**Option A â€” CLI**
```bash
# 1. Create target database
createdb ds_jobs

# 2. Create raw table
psql -d ds_jobs -c "
CREATE TABLE ds_jobs_raw (
  idx int,
  job_title text,
  salary_estimate text,
  job_description text,
  rating text,
  company_name text,
  location text,
  headquarters text,
  size text,
  founded int,
  type_of_ownership text,
  industry text,
  sector text,
  revenue text,
  competitors text
);"

# 3. Load raw data
\copy ds_jobs_raw FROM 'data/raw/Uncleaned_DS_jobs.csv' CSV HEADER

# 4. Run cleaning pipeline
psql -d ds_jobs -f sql/cleaning.sql

# 5. Run validation
psql -d ds_jobs -f sql/quality_checks.sql






## ğŸ“‚ Repository Structure
